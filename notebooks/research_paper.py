# -*- coding: utf-8 -*-
"""Research-paper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1piWL53K-g1mDeALyNi6hZeKINzu-7GYu
"""

!pip install PyPDF2

import os
import PyPDF2
import pandas as pd

# Define paths
reference_dir = '/content/drive/MyDrive/Research-paper/references'
papers_dir = '/content/drive/MyDrive/Research-paper/papers'
publishable_dir = reference_dir + '/Publishable'
non_publishable_dir = reference_dir + '/Non-Publishable'

# Define categories for publishable papers
categories = ['CVPR', 'EMNLP', 'KDD', 'NeurIPS', 'TMLR']

def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text() + "\n"
    return text

# Load labeled PDFs (Publishable and Non-Publishable)
labeled_data = []
for folder, label in [(publishable_dir, 'Publishable'), (non_publishable_dir, 'Non-Publishable')]:
    for subdir, _, files in os.walk(folder):
        for filename in files:
            if filename.endswith('.pdf'):
                text = extract_text_from_pdf(os.path.join(subdir, filename))
                category = os.path.basename(subdir) if label == 'Publishable' else None
                labeled_data.append({'Text': text, 'Label': label, 'Category': category})

# Load unlabeled PDFs
unlabeled_data = []
for filename in os.listdir(papers_dir):
    if filename.endswith('.pdf'):
        text = extract_text_from_pdf(os.path.join(papers_dir, filename))
        unlabeled_data.append({'Text': text, 'Label': None, 'Category': None})

# Combine data into DataFrame
data = pd.DataFrame(labeled_data + unlabeled_data)

# Dataset Exploration
print(data.info())
print(data.describe())
print(data['Label'].value_counts())

#Text Preprocessing
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import nltk
nltk.download('punkt_tab')
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    tokens = word_tokenize(text.lower())
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return ' '.join(tokens)

data['Processed_Text'] = data['Text'].apply(preprocess_text)

# Step 4: Feature Engineering - TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(max_features=1000)
X_tfidf = vectorizer.fit_transform(data['Processed_Text']).toarray()

# Step 4: Task 1 - Classify Publishable vs Non-Publishable
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Encode labels
data['Binary_Label'] = data['Label'].map({'Publishable': 1, 'Non-Publishable': 0})

# Split data
labeled_indices = data[data['Binary_Label'].notnull()].index
X_train, X_test, y_train, y_test = train_test_split(X_tfidf[labeled_indices], data.loc[labeled_indices, 'Binary_Label'], test_size=0.2, random_state=42)

# Train model
binary_model = RandomForestClassifier()
binary_model.fit(X_train, y_train)

# Predict for unlabeled data
data.loc[data['Binary_Label'].isnull(), 'Binary_Label'] = binary_model.predict(X_tfidf[data['Binary_Label'].isnull()])

data['Predicted_Label'] = data['Binary_Label'].map({1: 'Publishable', 0: 'Non-Publishable'})

# Step 5: Task 2 - Categorize Publishable Papers
from sklearn.preprocessing import LabelEncoder

# Filter publishable papers
publishable_data = data[data['Predicted_Label'] == 'Publishable']

# Encode categories
category_encoder = LabelEncoder()
publishable_data['Encoded_Category'] = category_encoder.fit_transform(publishable_data['Category'].fillna('Unknown'))

# Train model for categories
category_model = RandomForestClassifier()
category_model.fit(X_tfidf[publishable_data.index], publishable_data['Encoded_Category'])

# Predict categories for predicted publishable papers
data.loc[data['Predicted_Label'] == 'Publishable', 'Predicted_Category'] = category_encoder.inverse_transform(
    category_model.predict(X_tfidf[data['Predicted_Label'] == 'Publishable'])
)

# Step 6: Results
print(data[['Text', 'Predicted_Label', 'Predicted_Category']].head())

# Ratinale functions
def generate_publishability_rationale(processed_text, label):
    if label == 1:
        return "Contains innovative research concepts with clear methodology and results."
    else:
        return "Lacks novelty or has unclear objectives and methodology."

def generate_conference_rationale(processed_text, category):
    rationale_dict = {
        'cvpr': "Focuses on computer vision and pattern recognition topics.",
        'emnlp': "Discusses advancements in natural language processing.",
        'kdd': "Covers data mining and knowledge discovery methodologies.",
        'neurips': "Explores machine learning and artificial intelligence theories.",
        'tmlr': "Includes modern machine learning techniques and research."
    }
    return rationale_dict.get(category, "Not applicable.")

print(data.columns)

# Combined rationale for both publishability and conference classification
def generate_combined_rationale(row):
    if row['Publishable'] == 1:
        publish_rationale = generate_publishability_rationale(row['Processed_Text'], row['Publishable'])
        conf_rationale = generate_conference_rationale(row['Processed_Text'], row['Conference'])
        return f"{publish_rationale} {conf_rationale}"
    else:
        return generate_publishability_rationale(row['Processed_Text'], row['Publishable'])

# --- Classification Logic ---
# Step 1: Classify papers as Publishable (1) or Non-Publishable (0)
data['Publishable'] = data['Predicted_Label']

# Step 2: Categorize Publishable papers into 5 conferences
conference_categories = ['cvpr', 'emnlp', 'kdd', 'neurips', 'tmlr']
data['Conference'] = data.apply(lambda row: row['Predicted_Category'] if row['Publishable'] == 1 and row['Predicted_Category'].lower() in conference_categories else 'na', axis=1)


# Step 3: Generate Combined Rationale
data['Rationale'] = data.apply(generate_combined_rationale, axis=1)

# --- Prepare Results ---
data['Paper ID'] = [f"P{str(i+1).zfill(3)}" for i in range(len(data))]
result_df = data[['Paper ID', 'Publishable', 'Conference', 'Rationale']]

# --- Save Results to CSV ---
result_df.to_csv('/content/results.csv', index=False)
print("Results saved to results.csv")

